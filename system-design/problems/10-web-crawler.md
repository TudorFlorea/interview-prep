# Design Web Crawler

[â† Back to Problems](/system-design/problems/00-index.md)

---

## ğŸ¯ Problem Statement

Design a distributed web crawler that can crawl billions of web pages, extract content, and build a searchable index while respecting politeness policies.

**Difficulty**: ğŸ”´ Hard (Tier 1)

---

## 1. Requirements Gathering

### Functional Requirements

1. **Crawl web pages** - Fetch HTML content from URLs
2. **Extract links** - Discover new URLs to crawl
3. **Respect robots.txt** - Honor crawl restrictions
4. **Avoid duplicates** - Don't re-crawl same content
5. **Handle dynamic content** - JavaScript rendering (optional)
6. **Store content** - Save pages for indexing
7. **Prioritize URLs** - Crawl important pages first
8. **Incremental updates** - Re-crawl changed pages

### Non-Functional Requirements

| Aspect | Requirement |
|--------|-------------|
| **Fault Tolerance** | Continue on individual failures |
| **CAP** | AP - Eventual consistency |
| **Compliance** | Respect robots.txt, rate limits |
| **Scalability** | 10B pages, 1B pages/day |
| **Latency** | N/A (batch processing) |
| **Environment** | Global |
| **Durability** | Store crawled content |
| **Security** | Handle malicious sites |

---

## 2. Back of Envelope Calculations

```
Crawl Target:
- 10 billion pages total
- 1 billion pages/day
- 1B / 86400 â‰ˆ 11,500 pages/second

Page Sizes:
- Average page: 500 KB
- 1B pages Ã— 500 KB = 500 TB/day

Bandwidth:
- 500 TB / 86400 sec = 5.8 GB/sec = 46 Gbps

Storage (keep 1 copy):
- 10B pages Ã— 500 KB = 5 PB
- With compression (~5:1): 1 PB

URLs to Track:
- 10B URLs Ã— 100 bytes = 1 TB
```

---

## 3. Core Entities

```sql
-- URL Frontier (to be crawled)
CREATE TABLE url_frontier (
    url_hash BIGINT PRIMARY KEY,  -- Hash for dedup
    url TEXT NOT NULL,
    domain VARCHAR(255),
    priority INT,
    scheduled_at TIMESTAMP,
    last_crawled_at TIMESTAMP,
    crawl_count INT DEFAULT 0,
    
    INDEX idx_priority (priority DESC, scheduled_at)
);

-- Crawled Pages
CREATE TABLE crawled_pages (
    url_hash BIGINT PRIMARY KEY,
    url TEXT,
    content_hash BIGINT,  -- For dedup
    status_code INT,
    content_type VARCHAR(100),
    storage_path VARCHAR(500),
    crawled_at TIMESTAMP,
    
    INDEX idx_content_hash (content_hash)
);

-- Domain State
CREATE TABLE domains (
    domain VARCHAR(255) PRIMARY KEY,
    robots_txt TEXT,
    robots_fetched_at TIMESTAMP,
    crawl_delay_ms INT DEFAULT 1000,
    last_crawled_at TIMESTAMP,
    page_count INT DEFAULT 0
);
```

---

## 4. High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         WEB CRAWLER ARCHITECTURE                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                         URL FRONTIER                                 â”‚  â”‚
â”‚  â”‚                                                                       â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚  â”‚  â”‚                    Priority Queues                            â”‚   â”‚  â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚   â”‚  â”‚
â”‚  â”‚  â”‚  â”‚Priority â”‚  â”‚Priority â”‚  â”‚Priority â”‚  â”‚Priority â”‚         â”‚   â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ High    â”‚  â”‚ Medium  â”‚  â”‚  Low    â”‚  â”‚Re-crawl â”‚         â”‚   â”‚  â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â”‚   â”‚  â”‚
â”‚  â”‚  â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚   â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚  â”‚                                    â”‚                                 â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚  â”‚  â”‚                 Domain Rate Limiter                           â”‚   â”‚  â”‚
â”‚  â”‚  â”‚         (Ensures politeness per domain)                       â”‚   â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                       â”‚                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                         CRAWLER WORKERS                              â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚  â”‚
â”‚  â”‚  â”‚ Worker 1 â”‚  â”‚ Worker 2 â”‚  â”‚ Worker 3 â”‚  â”‚ Worker N â”‚   ...      â”‚  â”‚
â”‚  â”‚  â”‚  Fetch   â”‚  â”‚  Fetch   â”‚  â”‚  Fetch   â”‚  â”‚  Fetch   â”‚            â”‚  â”‚
â”‚  â”‚  â”‚  Parse   â”‚  â”‚  Parse   â”‚  â”‚  Parse   â”‚  â”‚  Parse   â”‚            â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜            â”‚  â”‚
â”‚  â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                      â”‚                                     â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚     â”‚                                â”‚                                â”‚    â”‚
â”‚     â–¼                                â–¼                                â–¼    â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚ â”‚  Content  â”‚                  â”‚   Link    â”‚                  â”‚   Dedup   â”‚â”‚
â”‚ â”‚   Store   â”‚                  â”‚ Extractor â”‚                  â”‚  Service  â”‚â”‚
â”‚ â”‚   (S3)    â”‚                  â”‚           â”‚                  â”‚           â”‚â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                      â”‚                                     â”‚
â”‚                                      â–¼                                     â”‚
â”‚                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚                               â”‚    URL    â”‚                               â”‚
â”‚                               â”‚  Filter   â”‚â—„â”€â”€â”€â”€ robots.txt check         â”‚
â”‚                               â”‚           â”‚â—„â”€â”€â”€â”€ seen URL check           â”‚
â”‚                               â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                                     â”‚                                      â”‚
â”‚                                     â–¼                                      â”‚
â”‚                              Back to Frontier                              â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. Deep Dive: URL Frontier

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   URL FRONTIER                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  The frontier manages URLs to be crawled with:                 â”‚
â”‚  â€¢ Prioritization (which URLs to crawl first)                  â”‚
â”‚  â€¢ Politeness (don't overwhelm any single domain)              â”‚
â”‚                                                                 â”‚
â”‚  Architecture:                                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                   Front Queues                          â”‚    â”‚
â”‚  â”‚              (Prioritization)                           â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”              â”‚    â”‚
â”‚  â”‚  â”‚ High â”‚  â”‚Mediumâ”‚  â”‚ Low  â”‚  â”‚Recrawlâ”‚              â”‚    â”‚
â”‚  â”‚  â”‚  F1  â”‚  â”‚  F2  â”‚  â”‚  F3  â”‚  â”‚  F4  â”‚              â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”€â”˜              â”‚    â”‚
â”‚  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚    â”‚
â”‚  â”‚                    â”‚                                   â”‚    â”‚
â”‚  â”‚                    â–¼                                   â”‚    â”‚
â”‚  â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚    â”‚
â”‚  â”‚            â”‚   Selector   â”‚  (Weighted random)        â”‚    â”‚
â”‚  â”‚            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚    â”‚
â”‚  â”‚                   â”‚                                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                      â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                  Back Queues                           â”‚    â”‚
â”‚  â”‚              (Per-domain politeness)                   â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”              â”‚    â”‚
â”‚  â”‚  â”‚domainâ”‚  â”‚domainâ”‚  â”‚domainâ”‚  â”‚domainâ”‚              â”‚    â”‚
â”‚  â”‚  â”‚  A   â”‚  â”‚  B   â”‚  â”‚  C   â”‚  â”‚  D   â”‚  ...        â”‚    â”‚
â”‚  â”‚  â”‚next: â”‚  â”‚next: â”‚  â”‚next: â”‚  â”‚next: â”‚              â”‚    â”‚
â”‚  â”‚  â”‚12:00 â”‚  â”‚12:01 â”‚  â”‚12:00 â”‚  â”‚12:02 â”‚              â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                 â”‚
â”‚  Priority Signals:                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                             â”‚
â”‚  â€¢ PageRank score                                              â”‚
â”‚  â€¢ Domain authority                                            â”‚
â”‚  â€¢ Freshness (how recently crawled)                            â”‚
â”‚  â€¢ Link depth (home page > deep pages)                         â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Frontier Implementation

```python
class URLFrontier:
    def __init__(self, redis: Redis):
        self.redis = redis
        
    def add_url(self, url: str, priority: int):
        """Add URL to frontier with priority"""
        url_hash = hash_url(url)
        domain = extract_domain(url)
        
        # Check if already seen
        if self.redis.sismember("seen_urls", url_hash):
            return False
            
        # Add to seen set
        self.redis.sadd("seen_urls", url_hash)
        
        # Add to priority queue (front queue)
        self.redis.zadd(f"frontier:priority:{priority}", 
                       {url: time.time()})
        
        # Add to domain queue (back queue)
        self.redis.rpush(f"frontier:domain:{domain}", url)
        
        return True
        
    def get_next_url(self) -> Optional[str]:
        """Get next URL respecting politeness"""
        
        # Get domains that are ready (past their delay)
        ready_domains = self.get_ready_domains()
        
        for domain in ready_domains:
            url = self.redis.lpop(f"frontier:domain:{domain}")
            if url:
                # Update last crawl time for domain
                self.redis.set(f"domain:last_crawl:{domain}", 
                              time.time())
                return url
                
        return None
        
    def get_ready_domains(self) -> List[str]:
        """Get domains whose delay has elapsed"""
        now = time.time()
        ready = []
        
        for domain in self.get_active_domains():
            last_crawl = self.redis.get(f"domain:last_crawl:{domain}")
            delay = self.get_crawl_delay(domain)
            
            if last_crawl is None or (now - float(last_crawl)) >= delay:
                ready.append(domain)
                
        return ready
```

---

## 6. Deep Dive: Politeness

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   POLITENESS POLICY                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  robots.txt Parsing:                                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                          â”‚
â”‚  # Example robots.txt                                           â”‚
â”‚  User-agent: *                                                  â”‚
â”‚  Disallow: /admin/                                              â”‚
â”‚  Disallow: /private/                                            â”‚
â”‚  Crawl-delay: 2                                                 â”‚
â”‚                                                                 â”‚
â”‚  User-agent: Googlebot                                          â”‚
â”‚  Allow: /                                                       â”‚
â”‚                                                                 â”‚
â”‚  Implementation:                                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚  1. Fetch robots.txt before crawling new domain                â”‚
â”‚  2. Cache for 24 hours                                         â”‚
â”‚  3. Parse Disallow/Allow rules                                 â”‚
â”‚  4. Extract Crawl-delay                                        â”‚
â”‚  5. Check each URL against rules before fetching               â”‚
â”‚                                                                 â”‚
â”‚  Rate Limiting:                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                â”‚
â”‚  â€¢ Default: 1 request per second per domain                    â”‚
â”‚  â€¢ Honor Crawl-delay if specified                              â”‚
â”‚  â€¢ Track last request time per domain                          â”‚
â”‚                                                                 â”‚
â”‚  Additional Politeness:                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                        â”‚
â”‚  â€¢ Identify crawler in User-Agent                              â”‚
â”‚  â€¢ Provide contact info                                        â”‚
â”‚  â€¢ Don't follow infinite loops (calendar URLs, etc.)          â”‚
â”‚  â€¢ Limit depth per domain                                      â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 7. Deep Dive: Deduplication

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DEDUPLICATION                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Two Types of Duplicates:                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                      â”‚
â”‚  1. URL-level: Same URL seen before                            â”‚
â”‚  2. Content-level: Different URLs, same content                â”‚
â”‚                                                                 â”‚
â”‚  URL Deduplication:                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                            â”‚
â”‚  â€¢ Normalize URLs (lowercase, remove fragments, sort params)  â”‚
â”‚  â€¢ Store 64-bit hash in Bloom filter                          â”‚
â”‚  â€¢ 10B URLs Ã— 8 bytes = 80 GB (fits in memory)                â”‚
â”‚                                                                 â”‚
â”‚  Content Deduplication:                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                        â”‚
â”‚  â€¢ Compute content hash (MD5/SHA1 of body)                     â”‚
â”‚  â€¢ SimHash for near-duplicate detection                        â”‚
â”‚                                                                 â”‚
â”‚  SimHash Algorithm:                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                            â”‚
â”‚  1. Extract features (words, shingles)                         â”‚
â”‚  2. Hash each feature to 64-bit                                â”‚
â”‚  3. For each bit position:                                     â”‚
â”‚     - If bit is 1, add weight                                  â”‚
â”‚     - If bit is 0, subtract weight                             â”‚
â”‚  4. Final hash: positive â†’ 1, negative â†’ 0                    â”‚
â”‚                                                                 â”‚
â”‚  Near-duplicates have similar SimHash (Hamming distance < 3)  â”‚
â”‚                                                                 â”‚
â”‚  Bloom Filter:                                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚
â”‚  â€¢ Space-efficient probabilistic data structure                â”‚
â”‚  â€¢ 1% false positive rate with 10 bits per element            â”‚
â”‚  â€¢ No false negatives                                          â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 8. Crawler Worker

```python
class CrawlerWorker:
    def __init__(self):
        self.frontier = URLFrontier()
        self.content_store = S3Client()
        self.robots_cache = RobotsCache()
        
    async def run(self):
        while True:
            url = self.frontier.get_next_url()
            if not url:
                await asyncio.sleep(0.1)
                continue
                
            await self.crawl(url)
            
    async def crawl(self, url: str):
        domain = extract_domain(url)
        
        # Check robots.txt
        robots = await self.robots_cache.get(domain)
        if not robots.can_fetch(url):
            return
            
        try:
            # Fetch page
            response = await self.fetch(url, timeout=30)
            
            if response.status_code != 200:
                self.handle_error(url, response.status_code)
                return
                
            # Check for duplicates
            content_hash = hash_content(response.body)
            if self.is_duplicate_content(content_hash):
                return
                
            # Store content
            storage_path = self.content_store.store(
                url, response.body, response.headers
            )
            
            # Extract and queue links
            if 'text/html' in response.headers.get('Content-Type', ''):
                links = self.extract_links(url, response.body)
                for link in links:
                    priority = self.calculate_priority(link)
                    self.frontier.add_url(link, priority)
                    
        except Exception as e:
            self.handle_error(url, str(e))
            
    async def fetch(self, url: str, timeout: int) -> Response:
        async with aiohttp.ClientSession() as session:
            async with session.get(
                url,
                timeout=timeout,
                headers={
                    'User-Agent': 'MyCrawler/1.0 (+https://example.com/bot)',
                    'Accept': 'text/html'
                }
            ) as response:
                body = await response.read()
                return Response(
                    status_code=response.status,
                    headers=dict(response.headers),
                    body=body
                )
```

---

## 9. Handling Scale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   SCALING STRATEGIES                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Distributed Crawling:                                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                         â”‚
â”‚  â€¢ Partition URLs by domain hash                               â”‚
â”‚  â€¢ Each worker handles subset of domains                       â”‚
â”‚  â€¢ Ensures politeness without coordination                     â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚           Domain Partitioning                       â”‚        â”‚
â”‚  â”‚                                                     â”‚        â”‚
â”‚  â”‚  Worker 1: domains where hash(domain) mod 10 = 0   â”‚        â”‚
â”‚  â”‚  Worker 2: domains where hash(domain) mod 10 = 1   â”‚        â”‚
â”‚  â”‚  ...                                                â”‚        â”‚
â”‚  â”‚  Worker 10: domains where hash(domain) mod 10 = 9  â”‚        â”‚
â”‚  â”‚                                                     â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                 â”‚
â”‚  Multi-Region Crawling:                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                        â”‚
â”‚  â€¢ Deploy crawlers in multiple regions                         â”‚
â”‚  â€¢ Crawl local sites from local region                        â”‚
â”‚  â€¢ Reduces latency, respects data residency                   â”‚
â”‚                                                                 â”‚
â”‚  DNS Caching:                                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚
â”‚  â€¢ Cache DNS lookups (TTL-based)                               â”‚
â”‚  â€¢ Reduces DNS overhead significantly                          â”‚
â”‚                                                                 â”‚
â”‚  Connection Pooling:                                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â”‚
â”‚  â€¢ Keep-alive connections to frequently crawled domains        â”‚
â”‚  â€¢ Reduces TCP handshake overhead                              â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 10. Technology Choices

| Component | Technology | Rationale |
|-----------|------------|-----------|
| Frontier | Redis + RocksDB | Fast queues + persistence |
| Content Store | S3 / HDFS | Scalable blob storage |
| Seen URLs | Bloom Filter + Redis | Memory efficient dedup |
| Metadata | Cassandra | Write-heavy, partitioned |
| Crawlers | Python (asyncio) | High concurrency |
| Coordination | Kafka | URL distribution |

---

## 11. Key Takeaways

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   KEY TAKEAWAYS                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. POLITENESS IS CRITICAL                                      â”‚
â”‚     Respect robots.txt and crawl delays                        â”‚
â”‚     Partition by domain for enforcement                        â”‚
â”‚                                                                 â”‚
â”‚  2. URL FRONTIER DESIGN                                         â”‚
â”‚     Front queues for priority                                  â”‚
â”‚     Back queues for per-domain rate limiting                  â”‚
â”‚                                                                 â”‚
â”‚  3. DEDUPLICATION                                               â”‚
â”‚     URL-level with Bloom filters                               â”‚
â”‚     Content-level with SimHash                                 â”‚
â”‚                                                                 â”‚
â”‚  4. HANDLE FAILURES GRACEFULLY                                  â”‚
â”‚     Retry with backoff                                         â”‚
â”‚     Don't get stuck on bad domains                            â”‚
â”‚                                                                 â”‚
â”‚  5. SCALE HORIZONTALLY                                          â”‚
â”‚     Partition by domain hash                                   â”‚
â”‚     Deploy multi-region                                        â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 12. References

- [18-rate-limiting.md](/system-design/fundamentals/18-rate-limiting.md) - Rate limiting
- [10-scaling-strategies.md](/system-design/fundamentals/10-scaling-strategies.md) - Scaling
- [09-message-queues.md](/system-design/fundamentals/09-message-queues.md) - URL distribution

---

[â† Back to Problems](/system-design/problems/00-index.md) | [Next: Ticketmaster â†’](/system-design/problems/11-ticketmaster.md)
